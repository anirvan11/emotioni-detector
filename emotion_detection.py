import cv2import numpy as npimport tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropoutfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.preprocessing.image import img_to_array, load_img# Define the model architecturedef create_model():    model = Sequential([        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),        MaxPooling2D((2, 2)),        Conv2D(64, (3, 3), activation='relu'),        MaxPooling2D((2, 2)),        Conv2D(128, (3, 3), activation='relu'),        MaxPooling2D((2, 2)),        Flatten(),        Dense(128, activation='relu'),        Dropout(0.5),        Dense(7, activation='softmax')    ])    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])    return model# Prepare the datadef prepare_data():    train_datagen = ImageDataGenerator(rescale=1./255)    test_datagen = ImageDataGenerator(rescale=1./255)        train_generator = train_datagen.flow_from_directory(        'archive/train',        target_size=(48, 48),        batch_size=32,        color_mode='grayscale',        class_mode='categorical'    )        validation_generator = test_datagen.flow_from_directory(        'archive/test',        target_size=(48, 48),        batch_size=32,        color_mode='grayscale',        class_mode='categorical'    )        return train_generator, validation_generator# Train the modeldef train_model(model, train_generator, validation_generator):    model.fit(        train_generator,        steps_per_epoch=train_generator.samples // train_generator.batch_size,        validation_steps=validation_generator.samples // validation_generator.batch_size,        validation_data=validation_generator,        epochs=25    )    model.save('emotion_model.h5')# Load the trained modeldef load_trained_model():    return tf.keras.models.load_model('emotion_model.h5')# Real-time emotion detectiondef emotion_detection_live(model):    emotion_dict = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}    cap = cv2.VideoCapture(0)        while True:        ret, frame = cap.read()        if not ret:            break                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)                for (x, y, w, h) in faces:            roi_gray = gray_frame[y:y+w, x:x+h]            roi_gray = cv2.resize(roi_gray, (48, 48))            roi_gray = roi_gray.astype('float') / 255.0            roi_gray = img_to_array(roi_gray)            roi_gray = np.expand_dims(roi_gray, axis=0)                        prediction = model.predict(roi_gray)            max_index = np.argmax(prediction[0])            predicted_emotion = emotion_dict[max_index]                        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)            cv2.putText(frame, predicted_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)                cv2.imshow('Emotion Detection', frame)        if cv2.waitKey(1) & 0xFF == ord('q'):            break        cap.release()    cv2.destroyAllWindows()if __name__ == '__main__':    # Uncomment these lines to train the model    # model = create_model()    # train_generator, validation_generator = prepare_data()    # train_model(model, train_generator, validation_generator)        # Load the trained model    model = load_trained_model()        # Start live emotion detection    emotion_detection_live(model)